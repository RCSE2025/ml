{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "effb53cb-d0ba-4532-b455-723937c2aa68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bolevard/ML/practice/nsfw_project/CLIP\n",
      "Obtaining file:///Users/bolevard/ML/practice/nsfw_project/CLIP\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: filelock in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: numpy in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.23.4)\n",
      "Requirement already satisfied: requests in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torchvision->clip==1.0) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from sympy->torch->clip==1.0) (1.2.1)\n",
      "Installing collected packages: clip\n",
      "  Attempting uninstall: clip\n",
      "    Found existing installation: clip 1.0\n",
      "    Uninstalling clip-1.0:\n",
      "      Successfully uninstalled clip-1.0\n",
      "  Running setup.py develop for clip\n",
      "Successfully installed clip-1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n",
      "/Users/bolevard/ML/practice/nsfw_project\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2108cf8a-9596-45b6-8b90-83d67bb576e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e99d6df-33a5-4c24-a44b-37d7c3eda267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097f1cd-9167-45f2-a40f-337de7d4da99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dfcd2a15-ca2e-4bf7-9886-a9aa3f21ee37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bolevard/ML/practice/nsfw_project/CLIP\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Obtaining file:///Users/bolevard/ML/practice/nsfw_project/CLIP\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: ftfy in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (4.64.1)\n",
      "Requirement already satisfied: torch in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (2.0.1)\n",
      "Requirement already satisfied: torchvision in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from clip==1.0) (0.15.2)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: filelock in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (3.10.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: numpy in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torchvision->clip==1.0) (1.23.4)\n",
      "Requirement already satisfied: requests in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from torchvision->clip==1.0) (9.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from jinja2->torch->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from requests->torchvision->clip==1.0) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bolevard/.pyenv/versions/3.8.13/envs/3_8_jupyter/lib/python3.8/site-packages (from sympy->torch->clip==1.0) (1.2.1)\n",
      "Installing collected packages: clip\n",
      "  Attempting uninstall: clip\n",
      "    Found existing installation: clip 1.0\n",
      "    Uninstalling clip-1.0:\n",
      "      Successfully uninstalled clip-1.0\n",
      "  Running setup.py develop for clip\n",
      "Successfully installed clip-1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n",
      "/Users/bolevard/ML/practice/nsfw_project\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " {'sensored/bigboys.jpg': {'drawings': 0.0012392222415655851,\n",
       "   'hentai': 0.0011606772895902395,\n",
       "   'neutral': 0.67901611328125,\n",
       "   'porn': 0.27151164412498474,\n",
       "   'sexy': 0.04707230255007744,\n",
       "   'violence': 0.5673182606697083},\n",
       "  'text': {'image_text_agressive': 0.014444471336901188,\n",
       "   'main_text_agressive': 0.10063949227333069}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytesseract\n",
    "import cv2\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoProcessor\n",
    "from torch import nn\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "from itertools import islice\n",
    "import warnings\n",
    "\n",
    "% cd\n",
    "CLIP\n",
    "!pip\n",
    "install - e.\n",
    "% cd..\n",
    "\n",
    "import CLIP.clip as clip\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "config = '-l rus+eng'\n",
    "path = ['sensored/bigboys.jpg']\n",
    "example_text_from_post = 'Тестовый текст'\n",
    "\n",
    "\n",
    "class Main:\n",
    "    def __init__(self, path_list, text_from_post):\n",
    "        self.emergency_flag = 0\n",
    "        self.text_from_post = text_from_post\n",
    "        self.path = './pytorch_nsfw_model-master/ResNet50_nsfw_model.pth'\n",
    "        self.classes = ['drawings', 'hentai', 'neutral', 'porn', 'sexy']\n",
    "        self.exceptions = ['drawings', 'neutral']\n",
    "        self.text_list = ['Photo without violence and killing a person or animal',\n",
    "                          'Photo with violence and killing of a person or animal or blood']\n",
    "\n",
    "        self.path_list = path_list\n",
    "        self.pil_images = [Image.open(x) for x in self.path_list]\n",
    "\n",
    "        self.img = [cv2.imread(x) for x in self.path_list]\n",
    "        self.img = [cv2.cvtColor(x, cv2.COLOR_BGR2RGB) for x in self.img]\n",
    "\n",
    "        self.transformation = transforms.Compose([transforms.Resize(224),\n",
    "                                                  transforms.CenterCrop(224),\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                       std=[0.229, 0.224, 0.225])\n",
    "                                                  ])\n",
    "\n",
    "        # ResNet 50 model for NSFW detecting\n",
    "        try:\n",
    "            self.image_model = models.resnet50()\n",
    "            self.image_model.fc = nn.Sequential(nn.Linear(2048, 512),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Dropout(0.2),\n",
    "                                                nn.Linear(512, 10),\n",
    "                                                nn.LogSoftmax(dim=1))\n",
    "            self.image_model.load_state_dict(torch.load(self.path, map_location=device))\n",
    "            self.image_model.eval()\n",
    "\n",
    "        except Exception:\n",
    "            warnings.warn(\"An error occurred while loading the ResNet model\")\n",
    "            self.emergency_flag = 1\n",
    "\n",
    "        # CLIP model for violence detecting\n",
    "        try:\n",
    "            self.clip = clip\n",
    "            self.clip_model, self.clip_transform = clip.load(\"ViT-B/32\", device=device)\n",
    "        except RuntimeError:\n",
    "            warnings.warn(\"An error occurred while loading the Clip model\")\n",
    "            self.emergency_flag = 1\n",
    "\n",
    "        # detecting text in an image using PyTesseract model\n",
    "        try:\n",
    "            self.string = ' '.join(\n",
    "                [pytesseract.image_to_string(x, config=config).replace('\\n', ' ').strip() for x in self.img])\n",
    "            self.texts = [self.string, self.text_from_post]\n",
    "        except Exception:\n",
    "            warnings.warn(\"An error occurred while using PyTesseract model\")\n",
    "            self.emergency_flag = 1\n",
    "\n",
    "        # NLP model for detecting aggression in text\n",
    "        try:\n",
    "            self.nlp_tokenizer = AutoTokenizer.from_pretrained('nlp_tokenizer/')\n",
    "            self.nlp_model = AutoModelForSequenceClassification.from_pretrained('nlp_model/').to(device)\n",
    "\n",
    "            self.tokenized_text = self.nlp_tokenizer(\n",
    "                self.texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            self.tokenized_text = dict(map(lambda x: (x[0], (x[1]).to(device)), self.tokenized_text.items()))\n",
    "\n",
    "        except Exception:\n",
    "            warnings.warn(\"An error occurred while loading the NLP model\")\n",
    "            self.emergency_flag = 1\n",
    "\n",
    "    def predict_image(self, image):\n",
    "        image_tensor = self.transformation(image).float()\n",
    "        image_tensor = image_tensor.unsqueeze_(0)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            image_tensor.cuda()\n",
    "\n",
    "        input_ = Variable(image_tensor)\n",
    "        output = self.image_model(input_)\n",
    "\n",
    "        return torch.softmax(output[:, :5], 1)\n",
    "\n",
    "    def get_logits(self, images_path_list, text):\n",
    "        clip_images = [Image.open(x) for x in images_path_list]\n",
    "        transformed_images = [self.clip_transform(x).unsqueeze(0).to(device) for x in clip_images]\n",
    "        tokenized_text = self.clip.tokenize(text).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = torch.cat([self.clip_model(image, tokenized_text)[0] for image in transformed_images], dim=0)\n",
    "        probs = torch.softmax(output, dim=1)[:, 1].cpu().tolist()\n",
    "        return probs\n",
    "\n",
    "    def predict(self):\n",
    "        self.image_dict = [self.predict_image(Image.open(x)) for x in self.path_list]\n",
    "        self.image_dict = dict(zip(path, [dict(zip(self.classes, x[0].float().tolist())) for x in self.image_dict]))\n",
    "        self.logits = self.nlp_model(\n",
    "            **self.tokenized_text\n",
    "        ).logits\n",
    "\n",
    "        violence = self.get_logits(self.path_list, self.text_list)\n",
    "        for name, proba in zip(self.path_list, violence):\n",
    "            self.image_dict[name]['violence'] = proba\n",
    "\n",
    "        self.image_dict['text'] = {}\n",
    "        self.image_dict['text']['image_text_agressive'], \\\n",
    "        self.image_dict['text']['main_text_agressive'] = torch.softmax(self.logits, 1)[:, 1].tolist()\n",
    "        return self.image_dict\n",
    "\n",
    "    def moderate(self):\n",
    "        if self.emergency_flag:\n",
    "            return -1, {}\n",
    "        flag = 0\n",
    "        final_dict = self.predict()\n",
    "        for name, dict_ in dict(islice(final_dict.items(), len(final_dict) - 1)).items():\n",
    "            for type_, proba in dict_.items():\n",
    "                if type_ not in self.exceptions:\n",
    "                    if proba > 0.6:\n",
    "                        flag = 1\n",
    "        if final_dict['text']['image_text_agressive'] > 0.6 or final_dict['text']['main_text_agressive'] > 0.6:\n",
    "            flag = 1\n",
    "        return flag, final_dict\n",
    "\n",
    "\n",
    "model = Main(path, example_text_from_post)\n",
    "model.moderate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b4959-6436-4fc1-934e-64f972cfafa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "44e1f1f9-4580-48aa-bda5-0b5764ef1b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4cbd2-0372-46ff-a80f-f59c289d7f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
